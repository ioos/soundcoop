{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d087f632",
   "metadata": {},
   "source": [
    "## Exercising PBP - PyPAM Based Processing\n",
    "\n",
    "**NOTE**: PBP is not a python _package_, so we will be cloning the PBP repo to get the code.\n",
    "\n",
    "PBP repo: https://github.com/mbari-org/pypam-based-processing\n",
    "\n",
    "In short, the main steps in this notebook are:\n",
    "\n",
    "- Clone PBP to support the HMB generation\n",
    "- Install dependencies, including PyPAM\n",
    "- Do preparations in terms of working space for downloaded and generated files\n",
    "- Generate HMB for a single day\n",
    "- Generate HMB for multiple days in parallel using Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b81f4-90c6-4910-8f63-f66ae0013afc",
   "metadata": {},
   "source": [
    "## Code preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d35799-009f-4c58-860d-af5a6f3bf9e8",
   "metadata": {},
   "source": [
    "### PBP clone\n",
    "\n",
    "We start by cloning the PBP repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e07b0-ac9a-431f-9af9-c3874089127d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Clone:\n",
    "!git clone https://github.com/mbari-org/pypam-based-processing.git\n",
    "\n",
    "### NOTE: Skip this cell if you have already got the clone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1afecf4-488f-4d0d-853e-6f59214ebe2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Change directories to the clone location:\n",
    "%cd pypam-based-processing\n",
    "\n",
    "### NOTE: You will need to execute this cell if re-running after a restart of the kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66faae3c-1363-470a-9c47-3f76a23fb42b",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeb3f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --no-cache-dir\n",
    "!pip install --no-cache-dir git+https://github.com/lifewatch/pypam.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54605247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This performs some basic PBP tests\n",
    "!python -m pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8dc4d-97c2-4e40-a4f7-845d7e716855",
   "metadata": {},
   "source": [
    "## Workspace preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f38450-5afd-44e8-a55e-9a3861c61fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our JSON and WAV input files for the demo are already located under these folders:\n",
    "!ls -l /home/jovyan/shared/readonly/data/mbari/pypam-based-processing/NB_SPACE/JSON/2022\n",
    "!ls -l /home/jovyan/shared/readonly/data/mbari/pypam-based-processing/NB_SPACE/DOWNLOADS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e432e0f-6628-4796-8398-2f2a14d2a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## So, this is a convenient definition we will use to instruct PBP where to get the input files from:\n",
    "INPUT_DIRECTORY = '/home/jovyan/shared/readonly/data/mbari/pypam-based-processing/NB_SPACE'\n",
    "\n",
    "## Generated netCDF and log files will be stored in this location:\n",
    "output_dir     = 'NB_SPACE/OUTPUT'\n",
    "\n",
    "## So, make sure that output folder exists:\n",
    "!mkdir -p NB_SPACE/OUTPUT\n",
    "\n",
    "## The name of generated files will be given this prefix:\n",
    "output_prefix  = 'MB05_'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb10e07-b1dd-4a18-979c-1289730746fa",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c1c775-5d74-45f7-be76-e82c1d35286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import xarray as xr\n",
    "import dask\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db17f66-c1a3-4b1a-a637-7af37eefe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path = ['.'] + sys.path\n",
    "from src.process_helper import ProcessHelper\n",
    "from src.file_helper import FileHelper\n",
    "from src.logging_helper import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058bd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## NOTE: The needed files are already downloaded for this demo,\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e4acf-b060-40d3-9009-f6c94d1e0d84",
   "metadata": {},
   "source": [
    "# A function to process a given day\n",
    "\n",
    "PBP includes these two main modules that we will be using below:\n",
    "\n",
    "- `FileHelper`, which facilitates input file reading (including from S3 buckets, though not really exercised in this notebook)\n",
    "- `ProcessHelper`, which is the main processing module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d440360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Supported by those PBP modules, we define a function that\n",
    "## takes care of processing a given day:\n",
    "\n",
    "def process_date(date: str, gen_netcdf: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to generate the HMB product for a given day.\n",
    "\n",
    "    It makes use of supporting elements in PBP in terms of logging,\n",
    "    file handling, and PyPAM based HMB generation.\n",
    "\n",
    "    :param date: Date to process, in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated\n",
    "    (see parallel execution below).\n",
    "\n",
    "    :return: the generated xarray dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    log_filename = f\"{output_dir}/{output_prefix}{date}.log\"\n",
    "\n",
    "    logger = create_logger(\n",
    "        log_filename_and_level=(log_filename, logging.INFO),\n",
    "        console_level=None,\n",
    "    )\n",
    "\n",
    "    ## Note: we use S3 URIs and boto as general mechanism to get our files from AWS.\n",
    "    ## We have already downloaded the necessary files for the demonstration.\n",
    "    ## The settings below allow us to still continue using the original S3 URIs without\n",
    "    ## triggering any new downloads.\n",
    "    s3_client = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "    file_helper = FileHelper(\n",
    "        logger=logger,\n",
    "        json_base_dir           = f'{INPUT_DIRECTORY}/JSON',\n",
    "        s3_client               = s3_client,\n",
    "        download_dir            = f'{INPUT_DIRECTORY}/DOWNLOADS',\n",
    "        assume_downloaded_files = True,\n",
    "        retain_downloaded_files = True,\n",
    "    )\n",
    "\n",
    "    process_helper = ProcessHelper(\n",
    "        logger=logger,\n",
    "        file_helper=file_helper,\n",
    "        gen_netcdf             = gen_netcdf,\n",
    "        output_dir             = output_dir,\n",
    "        output_prefix          = output_prefix,\n",
    "        global_attrs_uri       = 'metadata/mb05/globalAttributes_MB05.yaml',\n",
    "        variable_attrs_uri     = 'metadata/mb05/variableAttributes_MB05.yaml',\n",
    "        voltage_multiplier     = 1,\n",
    "        sensitivity_flat_value = 176,\n",
    "        subset_to              = (10, 24_000),\n",
    "        # max_segments=50  #TESTING\n",
    "    )\n",
    "\n",
    "    ## now, get the HMB result:\n",
    "    print(f'::: Started processing {date=}    {log_filename=}')\n",
    "    result = process_helper.process_day(date)\n",
    "\n",
    "    if gen_netcdf:\n",
    "        nc_filename = f\"{output_dir}/{output_prefix}{date}.nc\"\n",
    "        print(f':::   Ended processing {date=} =>  {nc_filename=}')\n",
    "    else:\n",
    "        print(f':::   Ended processing {date=} => (dataset generated in memory)')\n",
    "\n",
    "    if result is not None:\n",
    "        return result.dataset\n",
    "    else:\n",
    "        print(f'::: UNEXPECTED: no segments were processed for {date=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cee960-d5e4-4574-8dd3-f5f30d772869",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating the HMB products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c33ce9-61ed-4169-a976-cb721d472565",
   "metadata": {},
   "source": [
    "### Processing a day\n",
    "\n",
    "We can call the `process_date` function defined above directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7252d-45e2-46e8-9379-f072d5d674c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "generated_dataset = process_date('20220812')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'===> date completed. Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} mins)')\n",
    "\n",
    "generated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac16db-da76-4478-a89c-7d89ee900dc4",
   "metadata": {},
   "source": [
    "## Prepare process_date for parallel execution\n",
    "\n",
    "We will use [Dask](https://examples.dask.org/delayed.html) to dispatch multiple instances of `process_date` in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f820f99-e302-475d-b858-bb5dc806c8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_multiple_dates(dates: list[str], gen_netcdf: bool = False) -> list[xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Generates HMB for multiple days in parallel using Dask.\n",
    "    Returns the resulting HMB datasets.\n",
    "    \n",
    "    :param dates: The dates to process, each in YYYYMMDD format.\n",
    "\n",
    "    :param gen_netcdf:  Allows caller to skip the `.nc` creation here\n",
    "    and instead save the datasets after all days have been generated.\n",
    "\n",
    "    :return: the list of generated datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    @dask.delayed\n",
    "    def delayed_process_date(date: str):\n",
    "        return process_date(date, gen_netcdf=gen_netcdf)\n",
    "    \n",
    "    ## To display total elapsed time at the end the processing:\n",
    "    start_time = time.time()\n",
    "\n",
    "    ## This will be called by Dask when all dates have completed processing:\n",
    "    def aggregate(*datasets) -> list[xr.Dataset]:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'===> All {len(datasets)} dates completed. Elapsed time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} mins)')\n",
    "        return datasets\n",
    "\n",
    "\n",
    "    ## Prepare the processes:\n",
    "    delayed_processes = [delayed_process_date(date) for date in dates]\n",
    "    aggregation = dask.delayed(aggregate)(*delayed_processes)\n",
    "\n",
    "    ## And launch them:\n",
    "    return aggregation.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e2ffd-6f67-425c-bd2f-dfbd5dd702e9",
   "metadata": {},
   "source": [
    "### Processing multiple days\n",
    "\n",
    "We use the `process_multiple_dates` defined above to launch the generation of multiple HMB datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85cf420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The dates to process in this demo:\n",
    "dates = [\n",
    "    '20220812', '20220813',\n",
    "    '20220814', '20220815',\n",
    "    '20220816', '20220817',\n",
    "    '20220818', '20220819',\n",
    "    '20220820', '20220821',\n",
    "]\n",
    "\n",
    "## NOTE: due to issues observed when concurrently saving the resulting netCDF files,\n",
    "## this flag allows to postpone the saving for after all datasets have been generated:\n",
    "gen_netcdf = False\n",
    "\n",
    "## Get all HMB datasets:\n",
    "generated_datasets = process_multiple_dates(dates, gen_netcdf=gen_netcdf)\n",
    "\n",
    "print(f'Generated datasets: {len(generated_datasets)}\\n')\n",
    "\n",
    "if not gen_netcdf:\n",
    "    # so, we now do the file saving here:\n",
    "    print('Saving generated datasets...')\n",
    "    for date, ds in zip(dates, generated_datasets):\n",
    "        nc_filename = f'{output_dir}/{output_prefix}{date}.nc'\n",
    "        print(f'  Saving {nc_filename=}')\n",
    "        ds.to_netcdf(nc_filename,\n",
    "                     engine=\"netcdf4\",\n",
    "                     encoding={\n",
    "                        \"effort\": {\"_FillValue\": None},\n",
    "                        \"frequency\": {\"_FillValue\": None},\n",
    "                        \"sensitivity\": {\"_FillValue\": None},\n",
    "                     },\n",
    "        )\n",
    "\n",
    "print('\\nListing *.nc in OUTPUT folder:')\n",
    "!ls -l NB_SPACE/OUTPUT/*.nc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
