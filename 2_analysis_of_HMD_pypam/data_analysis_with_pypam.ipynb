{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dea1b7d-b275-4ff8-8290-115d1e4e58d1",
   "metadata": {},
   "source": [
    "# Pypam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a79f3-a9ee-4830-9377-00051a94415d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 0. Setup (install)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb634c1d-ec21-4d75-ae31-5c3b5af8d65b",
   "metadata": {},
   "source": [
    "First we need to install all the packages which we need to exectue this notebook. You don't need to do this if you're using mybinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bdf862-0a38-4b1f-844a-cc72a7ce5307",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pvlib # This one is optional for the summary plot! \n",
    "!pip install git+https://github.com/lifewatch/pypam@feature/hmb_to_octaves # install pypam's latest version (from github)\n",
    "!pip install minio # Install package to download the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d18bc-8a4a-46f4-aa19-65eb34b38a7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d8c854-5f3a-4236-80ab-84fe313ad9e6",
   "metadata": {},
   "source": [
    "We will download some processed HMB data stored in the cloud to give some examples of how can it be used. \n",
    "These data will be downloaded in this jupyterlab space, and you will be able to find them under the folder you specify here below, organized by station. \n",
    "Please change this line depending on where you want to store the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5000130d-ab67-4570-b15e-867df4e49859",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ced9a14-0b8f-408a-ac62-fe1ddfcbfc87",
   "metadata": {},
   "source": [
    "We will start defining a function to download data which can be then reused for different stations (then we don't need to write as much!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a412ce-8ca7-4193-ad3c-6158cad97945",
   "metadata": {},
   "source": [
    "We first start importing the packages we need for this part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53d66e0-53d7-4125-a654-6e872635a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import minio\n",
    "import os\n",
    "import pathlib\n",
    "from functools import partial\n",
    "import xarray\n",
    "import dask\n",
    "import pypam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7180d77-63f7-4576-ae16-166b2616eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dataset_cloud(object_name, data_folder, bucket, client, drop_variables):\n",
    "    object_data = client.get_object(bucket, object_name)\n",
    "    download_path = data_folder + '/' + object_name.replace('/', '_')\n",
    "    with open(str(download_path), 'wb') as file_data:\n",
    "        for data in object_data:\n",
    "            file_data.write(data)\n",
    "    ds = xarray.open_dataset(download_path, engine='netcdf4', drop_variables=drop_variables)\n",
    "    ds.attrs['download_path'] = download_path\n",
    "    ds.close()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217c57be-3348-4f25-9ed5-d0a0e7d7f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, freq_band, freq_coord, time_resample, data_vars, datetime_coord):\n",
    "    if freq_band is not None:\n",
    "        ds = pypam.utils.select_frequency_range(ds, freq_band[0], freq_band[1], freq_coord=freq_coord)\n",
    "    if time_resample is not None:\n",
    "        ds = ds.resample({datetime_coord: time_resample}).median()\n",
    "\n",
    "    if data_vars is not None:\n",
    "        ds = ds[data_vars]\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59480a30-95e1-4624-a2d5-7ebfa30fe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_mfdataset_cloud(\n",
    "        paths,\n",
    "        preprocess,\n",
    "        data_vars,\n",
    "        bucket,\n",
    "        client,\n",
    "        data_folder,\n",
    "        **kwargs,\n",
    "):\n",
    "    open_kwargs = dict(bucket=bucket, client=client, data_folder=data_folder, **kwargs)\n",
    "\n",
    "    # wrap the open_dataset, getattr, and preprocess with delayed\n",
    "    open_ = dask.delayed(open_dataset_cloud)\n",
    "    getattr_ = dask.delayed(getattr)\n",
    "    if preprocess is not None:\n",
    "        preprocess = dask.delayed(preprocess)\n",
    "\n",
    "    datasets = [open_(p, **open_kwargs) for p in paths]\n",
    "    closers = [getattr_(ds, \"_close\") for ds in datasets]\n",
    "    remover_ = dask.delayed(os.remove)\n",
    "    if preprocess is not None:\n",
    "        datasets = [preprocess(ds) for ds in datasets]\n",
    "\n",
    "    removers = [remover_(ds.attrs['download_path']) for ds in datasets]\n",
    "    \n",
    "    datasets, closers = dask.compute(datasets, closers, removers)\n",
    "\n",
    "    # Combine all datasets, closing them in case of a ValueError\n",
    "    combined = xarray.combine_by_coords(\n",
    "        datasets,\n",
    "        compat=\"no_conflicts\",\n",
    "        data_vars=data_vars,\n",
    "        coords=\"different\",\n",
    "        join=\"outer\",\n",
    "        combine_attrs=\"override\",\n",
    "    )\n",
    "\n",
    "    combined.set_close(partial(xarray._multi_file_closer, closers))\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8e93cb-6468-4bac-a9cd-1e1360d91dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_station(data_folder, client_obj, bucket_str, prefix_str, year, data_vars, freq_coord,\n",
    "                      datetime_coord, freq_band, time_resample):\n",
    "    files_to_load = []\n",
    "    objects = list(client_obj.list_objects(bucket_str, prefix=prefix_str))\n",
    "    for i, obj in enumerate(objects):\n",
    "        object_name = obj.object_name\n",
    "        path_name = pathlib.Path(object_name).name\n",
    "        if (not path_name.startswith('.')) and path_name.endswith('.nc'):\n",
    "            if str(year) in path_name:\n",
    "                files_to_load.append(object_name)\n",
    "            else:\n",
    "                print('Ignored: ' + path_name)\n",
    "    partial_func = partial(preprocess, data_vars=data_vars, datetime_coord=datetime_coord,\n",
    "                           freq_band=freq_band, time_resample=time_resample, freq_coord=freq_coord)\n",
    "    ds_tot = open_mfdataset_cloud(files_to_load,\n",
    "                                  preprocess=partial_func,\n",
    "                                  data_vars=data_vars,\n",
    "                                  bucket=bucket,\n",
    "                                  client=client,\n",
    "                                  data_folder=data_folder,\n",
    "                                  drop_variables=['psd_image_colormap',\n",
    "                                                  'psd_image',\n",
    "                                                  'percentile_image',\n",
    "                                                  'percentile_image_colormap'])\n",
    "    return ds_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f11efc5-cdcb-4097-beb6-8ac238ac6e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def load_data_station_slow(data_folder, client_obj, bucket_str, prefix_str, year, data_vars, freq_coord,\n",
    "                      datetime_coord, freq_band, time_resample):\n",
    "    objects = list(client_obj.list_objects(bucket_str, prefix=prefix_str))\n",
    "    ds_total = xarray.Dataset()\n",
    "\n",
    "    list_to_remove = []\n",
    "    for i, obj in enumerate(tqdm(objects)):\n",
    "        object_name = obj.object_name\n",
    "        path_name = pathlib.Path(object_name).name\n",
    "\n",
    "        if (not path_name.startswith('.')) and path_name.endswith('.nc'):\n",
    "            if str(year) in path_name:\n",
    "                object_data = client.get_object(bucket, object_name)\n",
    "                download_path = data_folder + '/' + object_name.replace('/', '_')\n",
    "                with open(str(download_path), 'wb') as file_data:\n",
    "                    for data in object_data:\n",
    "                        file_data.write(data)\n",
    "                list_to_remove.append(download_path)\n",
    "            else:\n",
    "                print('Ignored: ' + path_name)\n",
    "            if i % 3 == 0: \n",
    "                partial_func = partial(preprocess, data_vars=data_vars, datetime_coord=datetime_coord,\n",
    "                                           freq_band=freq_band, time_resample=time_resample, freq_coord=freq_coord)\n",
    "                ds_10 = xarray.open_mfdataset(pathlib.Path(data_folder).glob('*.nc'), parallel=True, preprocess=partial_func, data_vars=data_vars)\n",
    "                ds_total = xarray.merge([ds_total, ds_10])\n",
    "                for d in list_to_remove: \n",
    "                    os.remove(d)\n",
    "                list_to_remove = []\n",
    "                \n",
    "\n",
    "    return ds_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc1a1d-0b2a-45a3-acb0-243c6d36fe83",
   "metadata": {},
   "source": [
    "Then we will download the data for 2 stations. \n",
    "For this workshop we have chosen MARS and NRS11, but feel free to change the station names here if you wish to play around with other stations from the same providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aae26c-aa33-4a35-9e84-87addbaa9134",
   "metadata": {},
   "source": [
    "Let's start with MARS. We first need to describe where the data are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc7bff3-9018-4f12-ae68-17bf44c26ab7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/711 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Set up the download for MARS data\n",
    "client = minio.Minio( \"s3.us-west-2.amazonaws.com\", secure=False)\n",
    "\n",
    "bucket = 'pacific-sound-spectra'\n",
    "prefix = '2021/'\n",
    "station = 'MARS'\n",
    "\n",
    "ds_mars = load_data_station_slow(data_folder=local_path, client_obj=client, bucket_str=bucket,\n",
    "                  prefix_str=prefix, year=2021, data_vars=['psd'], freq_coord='frequency',\n",
    "                  datetime_coord='time', freq_band=[0, 12000], time_resample='1h')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe896244-3c7f-46ea-b4ee-7bcccac8da22",
   "metadata": {},
   "source": [
    "Then we move to the NRS11 data. We repreat the same process just with different parameters.\n",
    "These are the data we can find at [sanctsound](https://console.cloud.google.com/storage/browser/noaa-passive-bioacoustic/sanctsound?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false), daily millidecade bands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b33518-6499-4423-8273-48c4f1544842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the download for NRS11 data \n",
    "client = minio.Minio('storage.googleapis.com')\n",
    "bucket = 'noaa-passive-bioacoustic'\n",
    "station = 'NRS11'\n",
    "prefix = 'soundcoop/%s/' % station\n",
    "download_data_station(station, client, bucket, prefix, local_path, year=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926ab3a-88aa-4019-a596-15daf05bd534",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Let's look at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b73b031-c13b-405b-b69b-92f2aa33a4ae",
   "metadata": {},
   "source": [
    "We will first import all the packes we need to execut this part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6e0dc-4672-4fe0-b842-e538655f28f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pypam modules \n",
    "import pypam.utils\n",
    "import pypam.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341c436-0002-4a6e-af0f-06a3463c00bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import other tools\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83ada0-1c0b-4d68-8039-d5eed5612b20",
   "metadata": {},
   "source": [
    "Let's have a look at one of these netcdf files! You can click on the document sign or the database sign once the dataset is shown here below to get extra information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7635f-fff3-4d79-934a-884ef9ab1bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select one of the daily files\n",
    "# you can browse through the daily netcdf files and select nay of the files we just downloaded, and copy the path here\n",
    "ds = xr.open_dataset(local_path + '/NRS11/NRS11_H5R6.1.5000_20191108_DAILY_MILLIDEC_MinRes_v2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e80e9-2b2e-4f4e-8f81-20011213a978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show the file information\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d74ba3-c05e-44df-93d5-b07928cbebe5",
   "metadata": {},
   "source": [
    "We can then plot the long-term spectrogram of this day, selecting only the frequencies that we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603e95d-b4fe-487b-bc99-e52f34423864",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the minimum and the maximum frequency\n",
    "min_freq = 10\n",
    "max_freq = 10000\n",
    "\n",
    "# Plot the LTSA of one day\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ds = pypam.utils.select_frequency_range(ds, min_freq=min_freq, max_freq=max_freq, freq_coord='frequency')\n",
    "pypam.plots.plot_ltsa(ds=ds, data_var='psd', log=True, save_path=None, ax=ax, show=False, freq_coord='frequency',\n",
    "                      time_coord='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a9c97-bae6-4aa3-bb36-f108a601cac6",
   "metadata": {},
   "source": [
    "We can also apply the quality flag to plot these data (only for data where it is available, not for MARS): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0cbafa-9602-43df-82fe-ac13d43f427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the LTSA of one day with the mask\n",
    "minimum_quality = 2 # you can change this value depending on what you're interested in seeing. Check the ds quality_flag properties to see what each value (1 to 4) means\n",
    "ds_masked = ds.copy() \n",
    "ds_masked['psd'] = ds_masked.psd.where(ds_masked.quality_flag < minimum_quality)\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pypam.plots.plot_ltsa(ds=ds_masked, data_var='psd', log=True, save_path=None, ax=ax, show=False, freq_coord='frequency',\n",
    "                      time_coord='time')\n",
    "ax.set_facecolor('black') # -> this allows us to see in black where the mask has been applied "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3de65-3415-4fa2-b1a4-8fd8f7338703",
   "metadata": {},
   "source": [
    "We can also specify the location and then the day/night patter will be shown. But then we need to install the pvblib package (already done at the beginning of the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab58266-1785-40a2-9897-40a32dc9b825",
   "metadata": {},
   "source": [
    "First we will get where the location is from the metadata in the netdf file. We can find where is the property of that name when we were looking into the data some cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4aba6c-5cd0-4741-a523-61cb025c71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_str = ds.geospatial_bounds.replace('(', '').replace(')', '').split(' ')[1:]\n",
    "location = [float(l) for l in location_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d0c14-d703-48f6-9a77-38c2358a6623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lon, Lat\n",
    "min_psd = 35 # in db\n",
    "max_psd = 120 # in db\n",
    "h = 1 # in db\n",
    "percentiles = [1, 5, 10, 50, 90, 99]\n",
    "pypam.plots.plot_summary_dataset(ds, percentiles, data_var='psd', time_coord='time', freq_coord='frequency',\n",
    "                                 min_val=min_psd, max_val=max_psd, show=True, log=True, save_path=None,\n",
    "                                 location=location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e851a9-6f0b-46af-b866-0881726a4fe0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Long-term plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7716c-0fa4-4bf8-b432-c1eff2aee452",
   "metadata": {},
   "source": [
    "In this section we will show some tools for resampling data and long-term plotting. For this, we can load all the computed daily millidecade bands in one xarray dataset for each station, and we will save the output as one file (a full year per station!). \n",
    "For this we will load the data using  the join_all_ds_output_deployment of pypam, which uses Dask parallel computing for fast access. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf9c2d-0e30-4409-9ad0-5caa421e7eba",
   "metadata": {},
   "source": [
    "We will load only a part of the data (in frequency range), because the MARS data has spectrum values up to 100 kHz, and we want to keep it light. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184717e-cc3f-4bf3-8043-5f99c615b0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RESAMPLE_RESOLUTION = '1h' # we could also set it to 1d for daily resolution for example\n",
    "min_freq = 10\n",
    "max_freq = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb80112-e139-4a3b-a8c0-37ffa3170486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only get data from 2021\n",
    "def load_data_from_station_year(station, year, data_vars):\n",
    "    deployment_path = pathlib.Path(local_path).joinpath(station)\n",
    "    print('loading station %s...' % station)\n",
    "    aggregated_ds = pypam.utils.join_all_ds_output_deployment(deployment_path, data_vars=data_vars, datetime_coord='time',\n",
    "                                                              join_only_if_contains='_%s' % year, load=True,\n",
    "                                                              parallel=False, freq_band=[min_freq, max_freq],\n",
    "                                                              freq_coord='frequency',\n",
    "                                                              drop_variables=['psd_image_colormap',\n",
    "                                                                              'psd_image',\n",
    "                                                                              'percentile_image',\n",
    "                                                                              'percentile_image_colormap'])\n",
    "    return aggregated_ds # this assigns an xarray dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d9defe-65d0-44fd-b374-68fb6803e2f1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ Be patient, loading two full years can take a while...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150060c-bcef-42d4-9e9f-00e65368b835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data for each station\n",
    "# Quality flag is only available for NRS11, not for MARS. We can decide to ignore it or load it by removing it from the data_vars list \n",
    "ds_collection = {}\n",
    "ds_collection['MARS'] = load_data_from_station_year('MARS', 2021, data_vars=['psd'])\n",
    "ds_collection['NRS11'] = load_data_from_station_year('NRS11', 2021, data_vars=['psd', 'quality_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9ac54-1171-4873-ba08-cfa8e5f7742d",
   "metadata": {},
   "source": [
    "### Plot the HMB as yearly long-term spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e153ccb8-d160-47ce-b727-0ee16a514b7c",
   "metadata": {},
   "source": [
    "For memory reasons, plotting 1-minute resolution spectrograms for a full year is not possible. Therefore, we need to downsample the data (by taking the median, because values are in db). We can downsample it to hours or days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279fa50-10c6-41ea-b9f9-ce0505bbeea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_to_plot = 'NRS11' # By changing these value all the plots below will be for the selected stations. Feel free to change this name to any of the loaded stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8841cba-e4c5-4d47-9f28-f887a62c03e6",
   "metadata": {},
   "source": [
    "First let's look at how the full dataset looks like (not resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb6707-96de-4005-8a0f-39cd2fffc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collection[station_to_plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9237a-19ab-4c99-87a3-8600287d9c1d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ Be patient, resampling a full year can take a little while...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf15078-7408-4b4f-9c63-155b94e8827a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Need to resample first, otherwise there is too much data to plot!\n",
    "ds_resampled = ds_collection[station_to_plot].resample(time=RESAMPLE_RESOLUTION).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be936f-0ac2-49e6-850c-3d0ecfcf897b",
   "metadata": {},
   "source": [
    "And this is the resampled dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d8eba-da91-4270-b2a1-dd8edaa7bbd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e67e5a-2d3f-45d4-9a37-028b578a8e53",
   "metadata": {},
   "source": [
    "We can plot the LTSA also for an entire year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4788afd-eaf8-4b50-84c4-65861b82e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pypam.plots.plot_ltsa(ds=ds_resampled, data_var='psd', log=True, ax=ax, show=False, freq_coord='frequency',\n",
    "                      time_coord='time')\n",
    "ax.set_title(station_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d9f69-8e15-4ddd-92ec-46a90ce5fea4",
   "metadata": {},
   "source": [
    "And if it is available, we can plot a mask with the quality flag. \n",
    "First we will resample the quality flag so any hour has the flag of its worst minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c103efe-c212-4747-932f-fc0ee4c3f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_quality = 2\n",
    "ds_mask = ds_collection[station_to_plot].quality_flag.resample(time=RESAMPLE_RESOLUTION).max()\n",
    "ds_mask['psd'] = ds_resampled.psd\n",
    "ds_mask['psd'] = ds_mask.psd.where(ds_mask < minimum_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28299e77-f9bb-4300-aa2a-02c781a653f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pypam.plots.plot_ltsa(ds=ds_mask, data_var='psd', log=True, ax=ax, show=False, freq_coord='frequency',\n",
    "                      time_coord='time')\n",
    "ax.set_title(station_to_plot)\n",
    "ax.set_facecolor('black') # -> allows us to see where the quality flag has been applied\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae5108-551d-478e-abd7-753f6b1ce70c",
   "metadata": {},
   "source": [
    "### Plot the SPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43a68f-8970-4747-b62b-cebafd7424b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, convert the HMB output to SPD like we did for the small wav files examples\n",
    "percentiles = [1, 10, 50, 90, 99]\n",
    "min_psd = 35 # in db\n",
    "max_psd = 120 # in db\n",
    "h = 1 # in db\n",
    "ds_spd = pypam.utils.compute_spd(ds_resampled, data_var='psd', percentiles=percentiles, min_val=min_psd, \n",
    "                                 max_val=max_psd, h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbc446-824a-453d-b5b6-650111a56d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the obtained output\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "pypam.plots.plot_spd(spd=ds_spd, log=True, save_path=None, ax=ax, show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe1a84a-6590-40f9-a307-6c065501c4ab",
   "metadata": {},
   "source": [
    "### Plot a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20417ac-d6db-4380-9077-8e428eb35218",
   "metadata": {},
   "source": [
    "We can also make the same summary plot for the entire year, not only for one day (but then we don't show day/night as it would be too busy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a60ed-c6e2-4dfe-9fa0-fe67906be7c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pypam.plots.plot_summary_dataset(ds_resampled, percentiles, data_var='psd', time_coord='time', freq_coord='frequency',\n",
    "                                 min_val=min_psd, max_val=max_psd, show=True, log=True, save_path=None,\n",
    "                                 location=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd56ed3-416a-4e9a-ae18-890a10199eb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Whale presence indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0da84-6083-4ba0-bc4c-b8af842d6b81",
   "metadata": {},
   "source": [
    "Indices are the ratio of mean power spectral density (PSD) within the frequency band of peak call energy to mean PSD at background frequencies.\n",
    "These peak and background frequencies will be illustrated in the summary plot and call index calculations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e51b70-4a64-4b3d-8ad9-77e5de1226b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the frequencies that are used to compute call indices\n",
    "bluepeak = np.array([42, 43])\n",
    "finpeak = np.array([20, 21])\n",
    "bluebackground = np.array([37, 50])\n",
    "finbackground = np.array([12, 34])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f214b6-b198-459a-a309-a206126d2647",
   "metadata": {},
   "source": [
    "We can plot a LTSA with a line showing these values, but we will resample it first to daily spectra to get daily indices. \n",
    "Be patient, resampling a full year can take a little while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ac462-b346-4b24-ae37-92f9d95a157d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "whale_station1 = 'MARS' # Choose the station you want to plot\n",
    "daily_resampled1 = ds_collection[whale_station1].resample(time='1D').median()  # Resample to daily spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6fb484-2178-4621-9ebb-f2615e8bbf04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "whale_station2 = 'NRS11' # Choose the second station you want to plot\n",
    "daily_resampled2 = ds_collection[whale_station2].resample(time='1D').median()  # Resample to daily spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6822df5-2b8e-4782-adb7-b05879b81b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Then, plot the LTSA\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "pypam.plots.plot_ltsa(ds=daily_resampled1, data_var='psd', log=True, ax=ax, show=False, freq_coord='frequency',\n",
    "                      time_coord='time')\n",
    "ax.axhline(np.mean(bluepeak),color='black',linestyle='--',linewidth=0.5)\n",
    "ax.axhline(np.mean(finpeak),color='black',linestyle='--',linewidth=0.5)\n",
    "ax.set_title('NRS11')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe4f4e-d464-48e3-a5d1-f46d62ec56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_whale_index(peak, background, daily_ds):\n",
    "    pk = daily_ds.sel(frequency=peak).psd.mean(dim='frequency')\n",
    "    bg = daily_ds.sel(frequency=background).psd.mean(dim='frequency')\n",
    "    CI = pk / bg\n",
    "    return CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f65f52-adea-4b38-834d-e7c297225395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the whale indices\n",
    "finCI1 = compute_whale_index(finpeak, finbackground, daily_resampled1)\n",
    "blueCI1 = compute_whale_index(bluepeak, bluebackground, daily_resampled1)\n",
    "finCI2 = compute_whale_index(finpeak, finbackground, daily_resampled2)\n",
    "blueCI2 = compute_whale_index(bluepeak, bluebackground, daily_resampled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a563a3-441c-4ef4-ad9b-6e1e4aad8daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the whale indices\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,6), sharex=True, sharey=True)\n",
    "ax[0].plot(daily_resampled1.time.values, blueCI1, '.', label='blue whale')\n",
    "ax[0].plot(daily_resampled1.time.values, finCI1, 'o', markerfacecolor='none', label='fin whale')\n",
    "ax[0].axhline(1, color='black', linestyle='--', linewidth=0.5)\n",
    "ax[0].set_facecolor('white')\n",
    "ax[0].set_title(whale_station1)\n",
    "\n",
    "ax[1].plot(daily_resampled2.time.values, blueCI2, '.', label='blue whale')\n",
    "ax[1].plot(daily_resampled2.time.values, finCI2, 'o', markerfacecolor='none', label='fin whale')\n",
    "ax[1].axhline(1, color='black', linestyle='--', linewidth=0.5)\n",
    "ax[1].set_facecolor('white')\n",
    "ax[1].set_title(whale_station2)\n",
    "\n",
    "plt.suptitle('Daily whale call index')\n",
    "plt.ylabel('Call index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9eeea-ce26-4cbd-81e6-51f0e5d10a27",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Daily patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf6e33-3136-43f9-9283-c41719c20647",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can plot the hourly data we have obtained for the entire year to see if there are any daily patterns which repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96543a-a2ea-4cae-9eb3-4068f89f58b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ds_freq = pypam.utils.freq_band_aggregation(ds_resampled, data_var='psd', aggregation_freq_band=[10, 20])\n",
    "pypam.plots.plot_daily_patterns_from_ds(ds=ds_freq, data_var='psd', show=True, datetime_coord='time', ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e63d099-e5ed-4ad7-b750-1007e0b31eba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Compare data between stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc14afe-8125-463d-90b8-65900b9fb213",
   "metadata": {},
   "source": [
    "We can compare the spectrums of two locations, by visualizing the median and the [10, 90] percentile spectrum of each location.\n",
    "We first need to resample it to hourly median resolution for faster plotting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a3b8c-774c-4b0d-b127-6ee1b64d996a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⚠️ Be patient, resampling a two stations can take even a longer while...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08100e2f-fca8-4464-9289-712143ab8b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spectral Density comparison\n",
    "ds_collection_resampled = {'NRS11': ds_collection['NRS11'].resample(time='1h').median(), \n",
    "                           'MARS': ds_collection['MARS'].resample(time='1h').median()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe5005b-b532-46bd-b089-f8185831847d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = pypam.plots.plot_multiple_spectrum_median(ds_collection_resampled, 'psd', percentiles='default', \n",
    "                                               frequency_coord='frequency',\n",
    "                                               log=True, save_path=None, \n",
    "                                               ax=ax, show=True, time_coord='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f720aeb-27c8-4b18-9a6a-dbda44c6ec1a",
   "metadata": {},
   "source": [
    "### Aggregations and boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c8fd9-746d-4ccf-b721-0209b02b3881",
   "metadata": {},
   "source": [
    "We can now plot the evolution of all the stations by calling the bin aggregation function. It will compute the median of all the values included in the frequency band specified.\n",
    "Then it will aggregate all the values of the dataset in time to match the specified time frequency. \n",
    "PyPAM has a function to plot the evolution of one or multiple stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29ecd1-d98f-4788-816f-d311ba367b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_time = 'M'  # h, D, W, or M (hour, day, week or month)\n",
    "aggregation_freq_band = [10, 200]\n",
    "mode = 'violin'  # It can be boxplot, violin or quantiles\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax = pypam.plots.plot_multiple_aggregation_evolution(ds_dict=ds_collection,\n",
    "                                                     data_var='psd',\n",
    "                                                     mode=mode,\n",
    "                                                     show=True,\n",
    "                                                     datetime_coord='time',\n",
    "                                                     aggregation_time=aggregation_time,\n",
    "                                                     freq_coord='frequency',\n",
    "                                                     aggregation_freq_band=aggregation_freq_band,\n",
    "                                                     ax=ax,\n",
    "                                                     alpha=0.5)\n",
    "ax.set_title('Median sound intensity in water from frequency band %s to %s Hz' %\n",
    "             (aggregation_freq_band[0], aggregation_freq_band[1]))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960437f7-95bb-4214-ad56-31d89d97b3f6",
   "metadata": {},
   "source": [
    "## 7. Conversions: from HMB to decidecade bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea8e4b-2c17-4f8a-9203-cafe8d6032b7",
   "metadata": {},
   "source": [
    "We will work only with one day of data, just to show it (it will be faster!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259735c-8f08-49fe-9ca8-d9bb5a89bf52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(local_path + '/MARS/MARS_20210316.nc')\n",
    "\n",
    "# Convert back to upa for the sum operations\n",
    "ds_decidecades = pypam.utils.hmb_to_decidecade(ds, 'psd', freq_coord='frequency', fs=256000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ede6b7-5e56-4ee1-b6d1-3b243d23bfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(ds_decidecades.frequency, ds_decidecades['psd'].isel(time=0), label='Decidecades')\n",
    "ax.plot(ds.frequency, ds['psd'].isel(time=0), label='HMB')\n",
    "plt.xscale('symlog')\n",
    "plt.xlim(left=10)\n",
    "ax.set_facecolor('white')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
