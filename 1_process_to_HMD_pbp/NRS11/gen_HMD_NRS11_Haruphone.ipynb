{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da3db34-252f-4e5a-8c02-209e57486478",
   "metadata": {
    "id": "6da3db34-252f-4e5a-8c02-209e57486478"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Open-source software to process PAM data stored in the cloud\n",
    "\n",
    "As oceanic passive acoustic monitoring (PAM) datasets grow in volume and global coverage, the challenges of data storage and processing also grow.  This Python notebook illustrates recent developments in open-source software that help to address these challenges by:\n",
    "1. pulling PAM metadata and data directly from a cloud storage platform into a data processing environment, and\n",
    "2. producing standard data products that succinctly describe the ocean soundscape and its variations using hybrid millidecade band (HMB) spectra (Martin et al., 2021).\n",
    "\n",
    "For illustration purposes here, we will process a single day of audio data using [PyPAM Based Processing (PBP)](https://github.com/mbari-org/pbp) to produce:\n",
    "* an essential quality-control plot\n",
    "* a daily netCDF file containing HMB spectra at 1 minute resolution\n",
    "* a summary plot of the HMB spectra.\n",
    "\n",
    "Batch processing of longer time-series can also be accomplished in a Python notebook using PBP, however more powerful deployment strategies are preferred for batch processing with PBP at scale.\n",
    "\n",
    "The monitoring location for this example is [NOAA Ocean Noise Reference Station NRS11](https://www.pmel.noaa.gov/acoustics/noaanps-ocean-noise-reference-station-network), in [Cordell Bank National Marine Sanctuary](https://cordellbank.noaa.gov/).  Data from this site are [stored in Google Cloud](https://console.cloud.google.com/storage/browser/noaa-passive-bioacoustic/nrs/audio/11/nrs_11_2019-2021;tab=objects?prefix=&forceOnObjectsSortingFiltering=false)."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Install PBP and fetch calibration and metadata files\n",
    "\n",
    "This step is only required when running in Colab.  You can skip this cell if you are running this notebook in your local environment or in Binder."
   ],
   "metadata": {
    "collapsed": false,
    "id": "b8710412506a5965"
   },
   "id": "b8710412506a5965"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!git clone --depth 1 --branch v1.5.1 https://github.com/mbari-org/pbp.git\n",
    "!mkdir -p metadata/calibration\n",
    "!mkdir -p metadata/attribute\n",
    "!cp pbp/NRS11/NRS11_H5R6_sensitivity_hms5kHz.nc  metadata/calibration/\n",
    "!cp pbp/NRS11/*  metadata/attribute/\n",
    "!pip install -e pbp"
   ],
   "metadata": {
    "id": "25788a06a3212ec7"
   },
   "id": "25788a06a3212ec7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9455e773-bd1b-417f-a2a9-dad880243166",
   "metadata": {
    "id": "9455e773-bd1b-417f-a2a9-dad880243166"
   },
   "source": [
    "# Generate and review temporal metadata\n",
    "\n",
    "Essential to reliable processing of any PAM data time-series, regardless of duration, is reliable temporal metadata.  To help ensure reliable processing, PBP interactively generates temporal metadata and allows the user to review it before proceeding with processing.  If the start times and durations of all relevant audio files are accurately specified, mapping of input audio data to output analysis products will be accurate.  If it is requrired to first correct known inaccuracies in available temporal metadata, PBP includes methods to apply such corrections.\n",
    "\n",
    "Let's extract temporal metadata from input audio files stored in the cloud, for a specified time period.  The recorder for this location stores temporal metadata for each audio file in a separate xml file, and we use these xml files here.  The code below will generate essential temporal metadata for a brief period and store it in daily files in [json format](https://www.json.org/json-en.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c16b3-6723-4885-a33f-7f35d2212bf7",
   "metadata": {
    "id": "751c16b3-6723-4885-a33f-7f35d2212bf7"
   },
   "outputs": [],
   "source": [
    "# Audio data input specifications\n",
    "flac_uri              = 'gs://noaa-passive-bioacoustic/nrs/audio/11/nrs_11_2019-2021/audio'   # cloud storage location for the input audio data\n",
    "flac_prefix           = 'NRS11'       # prefix for the audio files\n",
    "start_date           = '20201017'   # start date for temporal metadata extraction (YYYYMMDD)\n",
    "end_date             = '20201019'   # end date for temporal metadata extraction (YYYYMMDD)\n",
    "json_base_dir        = 'metadata/json' # location to store generated metadata in JSON format\n",
    "\n",
    "# Import package modules\n",
    "from pbp.meta_gen.gen_nrs import NRSMetadataGenerator\n",
    "from pbp.logging_helper import create_logger_info\n",
    "from datetime import datetime\n",
    "\n",
    " # a logger that only logs messages tagged as info to the console, for more verbose logging\n",
    "log = create_logger_info(f'soundtrap_{start_date}_{end_date}')\n",
    "\n",
    "# Convert the start and end dates to datetime objects\n",
    "start = datetime.strptime(start_date, \"%Y%m%d\")\n",
    "end = datetime.strptime(end_date, \"%Y%m%d\")\n",
    "\n",
    "# Create the metadata generator\n",
    "meta_gen = NRSMetadataGenerator(\n",
    "        log=log,\n",
    "        uri=flac_uri,\n",
    "        json_base_dir=json_base_dir,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        prefixes=[flac_prefix])\n",
    "\n",
    "# Generate the metadata - this will generate JSON files in the json_base_dir\n",
    "meta_gen.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f363a6-62b3-461e-be93-634a28832ca0",
   "metadata": {
    "id": "f9f363a6-62b3-461e-be93-634a28832ca0"
   },
   "source": [
    "## View daily json files\n",
    "When this processing completes (in about 5 seconds), you can find and view the temporal metadata files that were just created within the directory structure of the notebook.  You will see a directory structure similar to this:\n",
    "\n",
    "```\n",
    "metadata/json/\n",
    "└── 2020\n",
    "    ├── 20201017.json\n",
    "    ├── 20201019.json\n",
    "    ├── ...\n",
    "└── nrs_coverage_20201017_20201019.jpg\n",
    "```\n",
    "\n",
    "The command below will list all of the daily files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109332b-0b23-402f-99dd-9d6ece838b89",
   "metadata": {
    "id": "4109332b-0b23-402f-99dd-9d6ece838b89"
   },
   "outputs": [],
   "source": [
    "!ls -l {json_base_dir}/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6470c1-0b45-4855-ab3c-04b374e4d6b0",
   "metadata": {
    "id": "ac6470c1-0b45-4855-ab3c-04b374e4d6b0"
   },
   "source": [
    "Each daily json file provides the essential temporal metadata for each file that contains data for the day being processed.  \n",
    "\n",
    "We can view the contents of a daily json file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb1c92-363e-4bea-adb7-f1910b90c2e5",
   "metadata": {
    "id": "1dbb1c92-363e-4bea-adb7-f1910b90c2e5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load JSON data from a file and print it\n",
    "with open(f'{json_base_dir}/2020/20201018.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c993c5-3c0e-4b7c-bbe5-d29bdd516caa",
   "metadata": {
    "id": "08c993c5-3c0e-4b7c-bbe5-d29bdd516caa"
   },
   "source": [
    "## View daily temporal coverage plot\n",
    "The temporal metadata generation also creates a summary plot that shows the percentage of each day that has audio data coverage.\n",
    "\n",
    "In this example, the temporal metadata indicate that each of the days within this brief period was fully sampled.\n",
    "\n",
    "It is important that the person responsible for processing a time-series first ensure that temporal coverage of input audio data is as-expected for the period to be processed.  This essential screening can identify issues such as previously unknown data gaps resulting from incomplete copying of part of the recording time-series to the storage archive.  If this plot shows the expected coverage, processing can proceed."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='metadata/json/nrs_coverage_20201017_20201019.jpg'))"
   ],
   "metadata": {
    "id": "9c9863dfa4eda91d"
   },
   "id": "9c9863dfa4eda91d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d087f632",
   "metadata": {
    "id": "d087f632"
   },
   "source": [
    "# Specify other processing metadata\n",
    "\n",
    "We next specify other metadata related to the audio recorder and the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9whAGckWQYX",
   "metadata": {
    "id": "e9whAGckWQYX"
   },
   "outputs": [],
   "source": [
    "# hydrophone specifications\n",
    "voltage_multiplier   = 1.0       # no change to raw audio required to account for PkPk voltage\n",
    "sensitivity_uri      = 'metadata/calibration/NRS11_H5R6_sensitivity_hms5kHz.nc'  # hydrophone sensitivity\n",
    "subset_to            = (10, 2000)   # subset frequency band for output HMB spectra, recording @ 5 kHz\n",
    "\n",
    "# metadata files for output netCDF data products\n",
    "global_attrs_uri     = 'metadata/attribute/globalAttributes_NRS11.yaml'\n",
    "variable_attrs_uri   = 'metadata/attribute/variableAttributes_NRS11.yaml'\n",
    "\n",
    "# output file locations\n",
    "download_dir         = 'downloads'   # audio files are stored here while being processed\n",
    "output_dir           = 'output'      # directory to store daily netCDF results and plots\n",
    "output_prefix        = 'NRS11_'       # a prefix for the name of generate files\n",
    "\n",
    "# select a day to process (YYYYMMDD format)\n",
    "date = '20201018'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd1928-4d9b-4f03-8ccc-cb99d024df7c",
   "metadata": {
    "id": "c1cd1928-4d9b-4f03-8ccc-cb99d024df7c"
   },
   "source": [
    "# Process a day of audio data\n",
    "\n",
    "With complete metadata specification, we can proceed to producing a daily netCDF file of HMB spectra.  \n",
    "\n",
    "When processing large data audio archives at scale, processing steps simply occur in sequence.  For the purposes of illustration in this notebook, the main steps are detailed sequentially:\n",
    "- Import package modules\n",
    "- Create `HmbGen` object with the parameters\n",
    "- Generate the core data product: a netCDF file of 1-minute HMB spectra\n",
    "- Generate a summary plot from the netCDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb10e07-b1dd-4a18-979c-1289730746fa",
   "metadata": {
    "id": "9fb10e07-b1dd-4a18-979c-1289730746fa"
   },
   "source": [
    "## Import package modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a03fc1-b65c-4707-bd09-03860d3098c5",
   "metadata": {
    "id": "b1a03fc1-b65c-4707-bd09-03860d3098c5"
   },
   "outputs": [],
   "source": [
    "from pbp.simpleapi import HmbGen\n",
    "\n",
    "from pbp import get_pbp_version\n",
    "get_pbp_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bc058f-698a-4b92-b984-a8e4c02d3280",
   "metadata": {
    "id": "e2bc058f-698a-4b92-b984-a8e4c02d3280"
   },
   "outputs": [],
   "source": [
    "# In this notebook we fetch audio files from public GCS buckets:\n",
    "from google.cloud.storage import Client as GsClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9426c58-8870-411d-b746-883237688252",
   "metadata": {
    "id": "d9426c58-8870-411d-b746-883237688252"
   },
   "source": [
    "## Create an `HmbGen` object with the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd2dc9-4020-4c5e-956b-cdaae2b84f2d",
   "metadata": {
    "id": "5dcd2dc9-4020-4c5e-956b-cdaae2b84f2d"
   },
   "outputs": [],
   "source": [
    "hmb_gen = HmbGen()\n",
    "\n",
    "hmb_gen.set_json_base_dir(json_base_dir)\n",
    "hmb_gen.set_global_attrs_uri(global_attrs_uri)\n",
    "hmb_gen.set_variable_attrs_uri(variable_attrs_uri)\n",
    "hmb_gen.set_sensitivity(sensitivity_uri)\n",
    "hmb_gen.set_voltage_multiplier(voltage_multiplier)\n",
    "hmb_gen.set_subset_to(subset_to)\n",
    "\n",
    "hmb_gen.set_gs_client(GsClient.create_anonymous_client())\n",
    "\n",
    "hmb_gen.set_download_dir(download_dir)\n",
    "hmb_gen.set_output_dir(output_dir)\n",
    "hmb_gen.set_output_prefix(output_prefix)\n",
    "\n",
    "hmb_gen.set_print_downloading_lines(True)\n",
    "\n",
    "# ----- JUST FOR INITIAL CONVENIENCE: ------\n",
    "hmb_gen.set_retain_downloaded_files(True)\n",
    "hmb_gen.set_assume_downloaded_files(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c55fe-df17-439d-b234-0b8474ea3cf7",
   "metadata": {
    "id": "401c55fe-df17-439d-b234-0b8474ea3cf7"
   },
   "source": [
    "## Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8edc5-92bf-4f0d-943d-a0216471a3ff",
   "metadata": {
    "id": "4cd8edc5-92bf-4f0d-943d-a0216471a3ff"
   },
   "outputs": [],
   "source": [
    "error = hmb_gen.check_parameters()\n",
    "# A message is returned in case of any errors\n",
    "if error:\n",
    "    raise RuntimeError(f\"check_parameters returned:\\n{error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be81e60-c395-494f-9ef1-d5436b4fb6f0",
   "metadata": {
    "id": "9be81e60-c395-494f-9ef1-d5436b4fb6f0"
   },
   "source": [
    "## Generate HMB product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7252d-45e2-46e8-9379-f072d5d674c5",
   "metadata": {
    "id": "99d7252d-45e2-46e8-9379-f072d5d674c5"
   },
   "outputs": [],
   "source": [
    "result = hmb_gen.process_date(date)\n",
    "\n",
    "# The resulting NetCDF file should have been saved under the output directory.\n",
    "\n",
    "# Here we expose the generated dataset itself to see it displayed in the notebook:\n",
    "result.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a66a14-b80a-4ea2-b542-94dd9554a4b7",
   "metadata": {
    "id": "87a66a14-b80a-4ea2-b542-94dd9554a4b7"
   },
   "source": [
    "## Generate summary plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee09b9-3072-4d21-a88b-0dd12739c12c",
   "metadata": {
    "id": "caee09b9-3072-4d21-a88b-0dd12739c12c"
   },
   "source": [
    "We can use the `plot_dataset_summary` function in mbari-pbp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d45a4-c8e1-49fd-b2e9-57efc7e4c29f",
   "metadata": {
    "id": "ab0d45a4-c8e1-49fd-b2e9-57efc7e4c29f"
   },
   "outputs": [],
   "source": [
    "from pbp.plotting import plot_dataset_summary\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a7f24-0218-4cd8-96fb-92ce57d4ec79",
   "metadata": {
    "id": "c24a7f24-0218-4cd8-96fb-92ce57d4ec79"
   },
   "outputs": [],
   "source": [
    "nc_filename = 'output/NRS11_20201018.nc'\n",
    "ds = xr.open_dataset(nc_filename, engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70869b11-8473-4fbb-a0ca-17c585f56452",
   "metadata": {
    "id": "70869b11-8473-4fbb-a0ca-17c585f56452"
   },
   "outputs": [],
   "source": [
    "plot_dataset_summary(\n",
    "    ds,\n",
    "    lat_lon_for_solpos=(37.88, -123.45),\n",
    "    title='Location: NRS11, Monterey Bay National Marine Sanctuary, 37.88°N, 123.45°W',\n",
    "    ylim=subset_to,\n",
    "    cmlim=(56, 117),\n",
    "    show=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
